# Web-scraping et Analyse de sentiments sur Trip Advisor 


## Table of Contents

* [Avant-propos](#avant-propos)
* [Contenu du projet](#contenu)
  * [EDA](#eda)
  * [Dev](#dev)
* [Installation](#installation)
  * [Data](#data)
  * [Notebooks (pour EDA)](#notebooks)
  * [Dev](#dev)
* [Contact](#contact)

<br>

## Avant-propos
Ce projet vise √† extraire les donn√©es des restaurants de Paris sur Trip Advisor, √† les nettoyer et r√©aliser une EDA (analyse exploratoire de donn√©es). Pour cela, nous utiliserons diff√©rents √©lements vus en cours tels que les m√©thodes de Web-scraping (Selenium, BS4), PySpark, etc... 

## Contenu
Ce projet comprend :
- Un dossier contenant des scripts utilis√©s dans la premi√®re partie (scripts), avec des sous-dossiers selon la tache r√©alis√©e:
    * preprocessor: Preprocessing avec Spark
    * scraper: Fonctions de webscraping + projet Scrapy( plus adapt√© pour notre projet)
    * viz: Fonctions pour faire des graphes pour l'analyse exploratoire
- Un dossier comportant les notebooks: num√©rot√©s dans l'ordre d'ex√©cution (webscraping => processing => eda)
- Un projet Airflow
- Une application Streamlit pour exposer les r√©sultats de l'analyse.

Dans un premier temps, nous effectuons une partie tourn√©e exploration des donn√©es tandis que dans un second temps nous souhaitons privil√©gier la capacit√© de d√©ploiement de ce projet.

### EDA
- Utilisation de techniques de web scraping pour r√©cup√©rer les donn√©es tel que Selenium, BS4. Finalement, le package Scrapy sera choisi pour sa modularit√© et pour sa r√©silience aux probl√®mes de connexion.
-	Emploi de PySpark pour nettoyer les donn√©es et faire du feature engineering: traitement des types, de la g√©olocalisation des restaurants, mais surtout processing du texte issu des commentaires.
-	Nous r√©alisons une EDA (Exploratory Data Analysis) orient√©e NLP en analysant principalement les commentaires des utilisateurs (scores de polarit√©). Cette analyse sera √©tendue en syst√®me de recommandation par la suite.

### Dev
-	Nous adaptons les codes pour √™tre orient√©s d√©veloppement et cr√©ons une pipeline ETL Airflow qui interagit avec une base SQL.
-	Nous proposons le m√™me type de contenu pour l'analyse pr√©c√©dente.
-	Nous g√©n√©ralisons notre approche √† d'autres villes.

## Installation
Pour utiliser ce projet, vous devez clone ce repository en local et installer les requirements.
Pour le notebook de processing vous avez deux alternatives: 
- Lancer depuis googlecolab ou en local en veillant √† installer les √©lements suivants:
conda..
python  3.9


### Data

- Extraction des donn√©es: depuis le notebook webscraping ou en lan√ßant la CL:
```
cd "scripts/scraper/scrapy_tripadvisor_scraper/tripadvisor_scraper"
scrapy crawl restaurants_urls_scraper
scrapy crawl reviews_scraper
```
Les donn√©es seront situ√©es dans le dossier data √† la racine du projet.

- Nettoyage des donn√©es: depuis le notebook processing () ou bien lancer le script global_processor...
### Notebooks
-	Pour les notebooks, il vous suffit de les ex√©cuter simplement. Remarque : le notebook qui concerne le preprocessing ne peut √™tre ex√©cut√© exclusivement sur Google Colab pour l'instant (car utilisation de Pyspark).
-	Il faudra vous assurer d'avoir le fichier fetch_data.json et clean_data.json pour que les notebooks de processing et d'eda fonctionnent (respectivement).

### Dev
Bient√¥t disponible.


## Contact
* [Lucie Gabagnouüë∏](https://github.com/luciegaba) - Lucie.Gabagnou@etu.univ-paris1.fr
